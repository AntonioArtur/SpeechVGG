{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "from libs.speech_vgg import speechVGG\n",
    "from keras.models import Model\n",
    "from libs.data_generator import log_standardize, pad_spec\n",
    "import soundfile as sf\n",
    "from scipy.signal import stft\n",
    "from glob import glob\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/beckmann/SpeechVGG\n"
     ]
    }
   ],
   "source": [
    "cd ../.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the functions we will need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_to_input(audio_file, audio_len=None):\n",
    "    audio = sf.read(audio_file)\n",
    "    audio = audio[0]\n",
    "    if audio_len and len(audio) > audio_len:\n",
    "        i = np.random.randint(len(audio) - audio_len + 1)\n",
    "        audio = audio[i:i+audio_len]\n",
    "    f, t, seg_stft = stft(audio,\n",
    "                        window='hamming',\n",
    "                        nperseg=256,\n",
    "                        noverlap=128)\n",
    "    mag_spec = np.abs(seg_stft)\n",
    "    spec_tmp = np.swapaxes(mag_spec, 0, 1)\n",
    "    data_tmp = spec_tmp[..., np.newaxis]\n",
    "    data_tmp[:,:,0] = log_standardize(data_tmp[:,:,0])\n",
    "    data_tmp= np.delete(data_tmp, (128), axis=1)\n",
    "    return data_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clips_to_features_rand(clips, model, num_random_patches=1):\n",
    "    specs = []\n",
    "    for clip in clips:\n",
    "        for n in range(num_random_patches):\n",
    "            specs.append(audio_to_input(clip, 16200))\n",
    "    #specs = np.array([pad_spec(spec) for spec in specs])\n",
    "    specs = np.array([spec for spec in specs])\n",
    "    feats = model.predict(specs)\n",
    "    a, b, c, d = np.shape(feats)\n",
    "    feats_small = np.zeros((len(clips), b, c, d))\n",
    "    for i in range(0, len(feats), num_random_patches):\n",
    "        feat_tmp = np.mean(feats[i:i+num_random_patches], axis=0)\n",
    "        feats_small[int(i/num_random_patches), ...] = feat_tmp\n",
    "    return feats_small.reshape(len(clips), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_short_clips(clips, length=16200):\n",
    "    good_idxs = []\n",
    "    for i, clip in enumerate(clips):\n",
    "        audio = sf.read(clip)\n",
    "        audio = audio[0]\n",
    "        if len(audio)>=length:\n",
    "            good_idxs.append(i)\n",
    "    clips=np.array(noise_clips)[good_idxs]\n",
    "    return clips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model and build extractor\n",
    "Let's now load ur model and build an extractor that computes the output of the last pooling block as our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = speechVGG(\n",
    "            include_top=False,\n",
    "            input_shape=(128, 128, 1),\n",
    "            classes=3000, # not important\n",
    "            pooling=None,\n",
    "            weights='/home/beckmann/speechVGG_models/460hours_3000words_weights.19-0.28.h5',\n",
    "            transfer_learning=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature extractor\n",
    "sVGG_extractor = Model(inputs=model.input,\n",
    "                       outputs=model.get_layer('block5_pool').output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "Now we load the data and remove short clips of less than a second. All short clips are noise clips and this will make them to distinguishable to our model that pads clips that are shorter than a second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_clips  = glob('/home/beckmann/musan/music/*/*.wav')\n",
    "noise_clips  = glob('/home/beckmann/musan/noise/*/*.wav')\n",
    "speech_clips = glob('/home/beckmann/musan/speech/*/*.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_clips = remove_short_clips(noise_clips)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features\n",
    "Now we extract representations of each audio clip from our data. We randomly select three one second long patches in each clip and average the output of the feature extractor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting features\n",
    "features_music  = clips_to_features_rand(music_clips, sVGG_extractor, 3)\n",
    "features_speech = clips_to_features_rand(speech_clips, sVGG_extractor, 3)\n",
    "features_noise  = clips_to_features_rand(noise_clips, sVGG_extractor, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate features and build corresponding label list.\n",
    "features = np.concatenate((features_speech, features_music, features_noise))\n",
    "labels = np.concatenate((np.zeros(len(features_speech)), np.ones(len(features_music)), 2*np.ones(len(features_noise))))                                                                                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TSNE â€“ Data visualization\n",
    "Now we can visualize our obtained features reducing them to two dimensions with TSNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tsne\n",
    "tsne = TSNE(n_components=2).fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "clabels = np.concatenate((np.zeros(253), np.ones(len(features_speech)-253), 2*np.ones(len(features_music)), 3*np.ones(len(features_noise))))                                                                                   \n",
    "plt.scatter(tsne[clabels==0,0], tsne[clabels==0,1], label='speech-usgov', color=[0,0.3,0.7,1], s=15)\n",
    "plt.scatter(tsne[clabels==1,0], tsne[clabels==1,1], label='speech-librivox', color=[0,0.6,0.8,1], s=15)\n",
    "plt.scatter(tsne[clabels==2,0], tsne[clabels==2,1], label='music', color=[0.6,0.4,0.8,1], s=15)\n",
    "plt.scatter(tsne[clabels==3,0], tsne[clabels==3,1], label='noise', color=[0.2,0.8,0.3,1], s=15)\n",
    "plt.legend()\n",
    "\n",
    "plt.tick_params(\n",
    "    axis='x',\n",
    "    which='both',\n",
    "    bottom=False,\n",
    "    top=False,\n",
    "    labelbottom=False)\n",
    "plt.tick_params(\n",
    "    axis='y',\n",
    "    which='both',\n",
    "    left=False,\n",
    "    right=False,\n",
    "    labelleft=False)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(7, 6)\n",
    "plt.savefig('/home/beckmann/scatter.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "Finally we can use the features to classify into speech, music and noise using a logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.1, random_state=2)\n",
    "logreg = LogisticRegression(max_iter=10000, penalty='l2', C=10)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_test_predict = logreg.predict(X_test)\n",
    "accuracy_score(y_test, y_test_predict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
